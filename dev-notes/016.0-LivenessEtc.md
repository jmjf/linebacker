# Readiness, liveness and health check endpoints

## What and why?

A liveness endpoint responds to indicate the container (application) is alive. If it can receive and respond to a request, it's alive. If the server is shutting down, return a 500.

A readiness endpoint responds with 200/OK if the container is ready to receive requests. If not, return a 500.

I want the service to provide readiness and liveness endpoints so callers can check the service's status. I'll follow the [Kubernetes model](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-http-request) for these endpoints so the app is k8s ready if needed and because I don't see the point in redefining those standards. Nodeshift also provides [an overview](https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/healthchecks.md) in their reference architecture page.

I considered using a package for this but it's so simple, it isn't worth adding a dependency.

I also want a health endpoint or status endpoint that will provide some basic information like:

-  Start time, `process.uptime()`, `process.resourceUsage()`
-  Requests received, responded, successful (2xx), client failure (4xx), server failure (5xx), possibly by endpoint
   -  Average response times for the last minute, 15 minutes, hour, day would be nice to have but require a lot of data and overhead; best derived from logs
-  Server status (not ready, ready, shutting down), Dependency (services) status (available, not available)

To track request status, the health endpoint probably requires request/response middleware similar to request/response logging (and that may be a good place to hook it in).

For a more sophisticated solution use `Map`s and a wrapper function that takes a path-like name (process/uptime, process/resourceUsage, requests/received, modules/backup-requests/creates, etc.), where each node in the name is a key and pointing to a Map that contains subnodes. Given that companies are founded on building such solutions, I'll stick with some simple variables in an object. Maybe more sophisticated solutions later.

Other References:

-  https://developers.redhat.com/blog/2020/11/10/you-probably-need-liveness-and-readiness-probes#liveness_and_readiness_probes
-

## Possible problems

The AuthN/AuthZ middleware runs for all endpoints. Setting up a k8s probe to use bearer tokens requires some special setup and may be problematic, depending on the environment. For now, I'll require authN/authZ, but the authZ check will allow any client in the authZ table, so no special entitlements. In the future, I may need to change the middleware setup to skip auth on these endpoints. TBD.

## Plan

Use the path prefix `.../api/zpages` with `livez`, `readyz`, and `healthz`. The `healthz` endpoint may require a special scope, but for now I'll leave it open to any caller.

Build a basic version that supports `livez`, `readyz`, and `healthz`. For `healthz` only report start time, up time, and resource use.

Add dependency status

Add request counts
