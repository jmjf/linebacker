# Experimenting with Splunk logging

I want to know how to get logs from pino into Splunk.

## Plan

-  Get it running in docker
-  Import a file from stdout for a short test run of linebacker
-  See what I get
-  Figure out how to get linebacker stdout into Splunk directly

References

-  http://innovato.com/splunk/GettingStarted.htm
-  http://innovato.com/splunk/

## Setup

Use the Splunk Docker container image. https://hub.docker.com/r/splunk/splunk/

Add Splunk to `dev-ms.yml`. Reference: https://github.com/dennybritz/docker-splunk/tree/master/enterprise. Also docker image page.

Password must meet https://docs.splunk.com/Documentation/Splunk/latest/Security/Configurepasswordsinspecfile

## Generate test log

Change `pinologger` to be generic. I can `| npx pino-pretty` to get pretty prints if needed.

**COMMIT: REFACTOR: pinologger always outputs to console; pipe to pino-pretty if needed**

## Import

In the Splunk web UI, import the data. I found that it's a multi-step process, so needed to ensure I did all the steps. A few I had trouble figuring out.

But the end result is a JSON log index. It's very searchable and analyzable.

## Feed

Splunk HTTP Event Collector (HEC) seems to be the preferred route for sending data.

Start by setting it up in Splunk for linebacker-hec-001 (\_json source type) gives me token 2f1c989e-fbc5-41b6-8000-7509c8ca1d52

I think I may be able to send json to the `.../services/collector` endpoint [docs](https://docs.splunk.com/Documentation/Splunk/9.0.1/RESTREF/RESTinput#services.2Fcollector). Let's try sending an event with Insomnia. I've configured it on port 8068.

Need to enable HEC globally. On the [Data Inputs, HEC page](http://localhost:8060/en-US/manager/search/http-eventcollector#), Global Settings (button upper right), click enable (top button). I also turned SSL off. Save.

Sending a message failed with 400 + "No Data". I had to wrap it as below. Not sure source type matters

```json
{
	"sourcetype": "linebacker-hec",
	"event": {
		"level": 20,
		"levelNumber": 20,
		"levelText": "debug",
		"time": "2022-10-06T01:32:59.772Z",
		"pid": 28648,
		"hostname": "santou",
		"name": "linebacker",
		"eventName": "BackupRequestCreated",
		"aggregateName": "BackupRequest",
		"aggregateId": "GMNAZauRHujsfzuYlQ2no",
		"msg": "added event"
	}
}
```

I can also send an array of events with similar structure. And a query `index="linebacker-test"` returns the four events.

So, if I can write something that gets the log lines and sends them with that format, I should be able to stream data. Problem: I don't get the event's time in the log. Splunk is using the time added.

I can send to the `.../services/collector/raw` endpoint and it will honor the time in the event and I don't need to wrap it. So, if I can read the input stream and send it to the raw endpoint, it should work. I think pino-abstract-transport could help make this easier.

## Custom format

The Splunk logging GitHub has an example for [custom format](https://github.com/splunk/splunk-javascript-logging/blob/master/examples/custom_format.js) that sends data in a string not JSON.

Let's try that from Insomnia and see how it lands.

Created a new HEC lb-custom-01 with token fbe807ca-de88-483c-8f8c-2e9af181d1e5

That doesn't seem to be working posting to the event endpoint as advertised.

Can I use metadata and can I include custom metadata properties?

Created a new HEC lb-md-01 with token fbfb5f78-73db-4af0-9104-6ff8b74f5bf5, feeds lb-md-01 index with \_json source type

Created a custom source type based on \_json that recognizes time fields and the message below gets the time in Splunk correctly. Sending time as an ISO string fails. Entries in "fields" are extracted, which makes them easier to search (don't need to prefix them with a full object path), as are fields in the event outside the "message".

```json
{
	"source": "linebacker",
	"host": "santou",
	"sourcetype": "_json",
	"time": 1665020010.39,
	"fields": {
		"levelText": "info",
		"levelNumber": 30,
		"pid": 28648,
		"moduleName": "BackupRequestAllowedSubscriber.ts",
		"functionName": "onBackupRequestAllowed"
	},
	"event": {
		"severity": "info",
		"pid2": 28648,
		"message": {
			"level": 30,
			"pinotime": "2022-10-06T01:33:30.390Z",
			"backupRequestId": "GMNAZauRHujsfzuYlQ2no",
			"eventName": "BackupRequestAllowed",
			"resultType": "ok",
			"value": {
				"backupRequestId": "GMNAZauRHujsfzuYlQ2no",
				"backupJobId": "993aca31-453c-4c6c-878a-82551e2310a1",
				"dataDate": "2022-06-30T00:00:00.000Z",
				"preparedDataPathName": "data-location",
				"getOnStartFlag": true,
				"transportTypeCode": "HTTP",
				"backupProviderCode": "CloudA",
				"storagePathName": "storagePathName",
				"statusTypeCode": "Sent",
				"receivedTimestamp": "2022-10-06T01:32:59.770Z",
				"checkedTimestamp": "2022-10-06T01:32:59.889Z",
				"sentToInterfaceTimestamp": "2022-10-06T01:33:30.368Z",
				"requesterId": "tWSD9EUKe7MBt3sGDwbkGhFWVshIQrmk@clients"
			},
			"msg": "Use case ok 7"
		}
	}
}
```

So, define a base structure so I can build fields (either in fields or as event values outside the message. Or I can just shove the pino message object into the event and get all the outer fields directly searchable. The time needs to go outside the event to be used, though, and is significant when filtering results by time windows (Splunk looks at \_time, which comes from the time provided).

For practical purposes, though, a base structure makes sense and it should include:

-  levelNumber
-  levelName
-  moduleName
-  functionName
-  msg
-  name (of application) -> Splunk source
-  pid (if available)
-  traceId (if available)
-  value or error or other data detail that might be useful
   -  just be sure not to log anything sensitive

Ensure controllers log traceId in all outputs.

## For future planning

-  Build a pino splunk transport using `pino-abstract-transport` and Splunk's JS libraries and the [`SplunkLogger`](https://docs.splunk.com/DocumentationStatic/JSlogging/0.11.1/splunk-javascript-logging-docs/SplunkLogger.html) they provide
-  The `SplunkLogger.send()` method takes a message, severity, and metadata
   -  Metadata mapping: host -> host; name -> source
   -  Event -> pino object minus host, name, time (???)
   -  Time -> pino object time.valueOf()/1000
-  Also see [examples](https://github.com/splunk/splunk-javascript-logging/tree/master/examples)
   -  Auto-batching is probably wise if batch slices are kept small enough

**COMMIT: DOCS: notes Splunk logging and on how to route logs from pino to Splunk**
