# Pino transport to move data to Splunk

-  Build a pino splunk transport using `pino-abstract-transport` and Splunk's JS libraries and the [`SplunkLogger`](https://docs.splunk.com/DocumentationStatic/JSlogging/0.11.1/splunk-javascript-logging-docs/SplunkLogger.html) they provide
-  The `SplunkLogger.send()` method takes a message, severity, and metadata
   -  Metadata mapping: host -> host; name -> source
   -  Event -> pino object minus host, name, time (???)
   -  Time -> pino object time.valueOf()/1000
-  Also see [examples](https://github.com/splunk/splunk-javascript-logging/tree/master/examples)
   -  Auto-batching is probably wise if batch slices are kept small enough

## Setup

`npm install pino-abstract-transport splunk-logging`
`npm install --save-dev @types/splunk-logging`

## Build it

Start with the example for console logging from https://github.com/pinojs/pino-abstract-transport.

Build a `logToSplunk` method to replace `console.log()` using examples from `splunk-logging`.

Looking at the Splunk code, it gets `time` from `metadata`, but that isn't defined in the types, so can't type the payload with a defined type. See `_initializeMetadata` (accepts `time`) and `_makeBody` (formats time, note that `body.time` here is from `metadata`).

## Try it

Configure a HEC, `linebacker-pino-01` that sends to `linebacker-pino-01` index with type `_json`. Add token as default for the transport (for now). Search index and see empty.

Start backend components. `npm run express:dev:typeorm | npx ts-node src/infrastructure/logging/pinoSplunkTransport.ts`.

Does not seem to be working. Added some `console.log()`s and they aren't being called, so it doesn't seem to be working.

Let's try the example abstract transport. It isn't pipeable either. Looks like `pino-pretty` does things differently (transform stream described in pino transport docs, not in `pino-abstract-transport` docs), so let's try that.

After some digging, I need some stuff from `pino-pretty`'s `bin.js` -> `pstBin.ts`. A quick test with the call to log to Splunk disabled outputs to the console as expected. Time for the real test.

So, the command line is `npm run express:dev:typeorm | APP_ENV=dev npx ts-node src/infrastructure/logging/pstBin.ts`

Well, it logged to Splunk (yay!) but the log result isn't what I want.

-  Not honoring `time`
-  The log content I care about is in `event.message` and isn't searchable
-  So, I need to drop `splunk-logging`, build the event myself
   -  Reading through the `splunk-logging` code, it looks like it doesn't play nice with what my testing showed works
   -  It makes some assumptions that don't agree with what I see happening in the real data
   -  And what it sends for message is stringified JSON that isn't parsed in Splunk for some reason
   -  I'm guessing it may require specific configuration to in splunk to work
   -  So, I need to write my own caller to send data to Splunk's URL like I'm doing with Insomnia
   -  But, some of the concepts in the logger, like the queue manager, are worth basing on

So, I have a working transport, but I need to change how it sends data to Splunk.

**COMMIT: FEAT: build transport for Splunk data; gets data to Splunk, but isn't usable**

## Custom format?

This may be easier than I thought. After some reading, I think I may be able to use a custom formatter that spreads the message and add severity.

I see that pino is delivering strings to the stream, so I need to `JSON.parse()` them.

My formatter is formatting the message right, but I'm getting EPROTO SSL errors.

After a lot of chasing in circles, I found that, somehow, I had https for the url. Ugh! Now the code runs and the output looks okay and it searches nice in Splunk.

But I see stuff in the Splunk code that doesn't make a whole lot of sense to me--like initializing context in the send messages function, which means any error output shows something other than what is sent to the function, which misled me for a while too. It also has issues reported, including a bug that hasn't been resolved in 6 years. I think I'll rewrite the code.

I'm also seeing some issues where events seem to stop arriving and console output stops too. The server is still working (can see data in the database), but logs aren't visible in Splunk or on the console after about 14-15 logs.

The pipeline may be the problem. Testing with an example from the pino transport docs works as expected when logging to the console.

Yep. That solves the problem. And I have it writing to stdout, which lets me pipe to pino-pretty after the splunk transport.

**COMMIT: REFACTOR: use for...of model and basic cleanup**

Next steps: get this thing in shape; decide if I want to handle HTTPing Splunk myself and managing the log queue
