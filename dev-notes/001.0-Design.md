# 1.0 - Design and lay out infrastructure

## Project description

Before I start writing code, I want to think about what I'm building and why I'm building it, then outline how I'll build it and why I'll build it that way.

I need a system to manage backups. The main components of that system are:

-  UI - A user interface (web application) and supporting service (HTTP API) that allow users to create, review, and approve or deny jobs that can create backups. In this service, I have concepts like backup job (runs many times, creates point in time backups) and backup (point in time copy of data).
-  BZI - A backup zone interface service that knows how to create, delete and restore backups for a specific backup destination. For example, I might backup data in a separate database, a local file store, or in a cloud storage platform. Each of those destinations is a separate backup zone interface that presents a common interface over the storage technology specifics.
-  BC - A central backup controller that allows users to request a backup be created and stored. When a backup is created, the backup controller holds the data about all backups. The backup controller uses that data to decide when a backup can be deleted. It also accepts requests to restore a backup and tracks restored copies. For each backup action (create, delete, restore), the backup controller sends requests to the correct backup zone interface and accepts responses.

Some basic assumptions:

-  We may have more than one BZI.
-  Each BZI probably runs on the storage technology it supports.
   -  If it supports Cloud A, it probably runs on Cloud A.
   -  If it supports Cloud B, it probably runs on Cloud B.
   -  If it supports an on-premise storage technology, it probably runs on-premise.

![High level components of the backup system](img/BackupSystemComponents.png 'High level components of the backup system')

The backup controller has several, mostly independent paths. For now, I'll focus on the Create Backup function. I expect I'll have a Delete Backup function and a Restore Backup function.

This project is about the backup controller service. It should:

-  Receive backup requests to backup data over HTTP (and possibly other transports in the future)
-  Confirm the backup request is allowed per business rules (has an approved, in-force backup job defined)
-  Send allowed backup requests to a separate service that manages the backup zone (and add data needed by that service)
-  Log all backup requests and their state throughout the request lifecycle
-  Survive failure sanely (on startup, figure out where it left off and resume; when network connections fail, wait until they come back up and pickup where it left off)

### Many transports -> decouple logic

I may accept non-HTTP transports in the future (queue polling, etc.). I don't want to duplicate the controller code between transports, so I want a clear boundary between the service logic and the HTTP interface so different transports can call the same controller code.

### Confirm allowed -> call another service

I need information about backup jobs to confirm the request is allowed. The UI and it's supporting service own that data, so I need to call the UI service to get information about the backup job.

-  NOTE: The backup request needs to identify the backup job it's using.

### Track lifecycle -> persistent storage

I need to track request lifecycle. The time from receiving a request to getting back data about the backup BZI created may take minutes or hours, so I need some kind of persistent storage for requests. That persistent storage also lets me handle restarts by reading data about requests in specific states. The states are:

-  Received
-  Allowed/Not Allowed (mutually exclusive)
-  Sent/Not Sent (mutually exclusive)
-  Succeeded/Failed (mutually exclusive)

![Backup request states](img/BackupRequestStates.png 'Backup request states')

Not Allowed, Succeeded, and Failed are terminal states--the request lifecycle ends at those states.

I assume I'll want to track the time for each state transition. I'll also need a way to identify each request, so I'll assign an id to each request. I can return that request to the backup job, send it to the "get backup job data" call, and send it to the BZI so the backup system so I can have [distributed tracing](https://microservices.io/patterns/observability/distributed-tracing.html). The BZI will return the request id to the BC so the BC can reliably find the request for result handling.

> Why not use the backup job id for result handling?
> If I get two requests for the same backup job, I need more than the backup job id to identify the requests. I could get two requests for the same backup job if BC is delayed long enough or if I get an update (restatement) for a backup (backup job 123 running for 2021-01-01 data on 2021-01-08 and backup job 123 running for 2021-01-07 data running on 2021-01-08).

### Surviving failures

Thinking about recoverability, when the service starts, if a request is:

-  Received but not Allowed or Not Allowed, I should check if the request is allowed and continue the lifecycle. (Not Allowed must be an explicit state.)
-  Allowed but Not Sent, I should send the request to BZI. (Not Sent does not need an explicit state.)

If the call to get backup job data to check if the request is allowed fails, I need a way to retry with delay falling into a circuit breaker pattern or similar.

If the call to send to BZI fails, I need a way to retry with delay falling into a circuit breaker pattern or similar.

BZI may take hours to respond with a result, so I need a way to get results that survives the backup controller service failing. My best option is some kind of temporary data store to which BZI writes results and from which the Create Bacbkup component reads results. Create Backup will use the results data to:

-  Write the backup information to the master store of backups (if successful)
-  Update the request's state
-  Delete the result

If the service fails mid-processing, the result remains on the temporary store, so BC can get it and try again when it restarts. The write to the master store of backups and state needs to be idempotent to avoid duplicates if Create Backup fails after either of the first two steps.

### Why wouldn't the BZI own the master store of backups?

We expect many BZIs because a BZI supports a specific backup storage technology--file, database, cloud (and different clouds), etc. If each BZI owns a separate master store of backups, there is no common view of all backups.

-  The UI needs to show a list of backups for a backup job. That requires the UI to call the appropriate BZI for each backup job when it wants to show users a list of backups. (The UI must know about the BZIs.)
-  Each BZI must present an HTTP API the UI can call and must respond in reasonable (<5 seconds) time. This function is overhead that doesn't fit the BZI's purpose of providing an common interface to manage backup objects on a given technology.
-  If the BC doesn't have a master store of backups, the Delete Backup job is memory and CPU intensive.
   -  To decide which backups can be deleted, BC's Delete Backup job needs a list of all backups and their delete dates.
   -  Delete Backup must call each BZI to get a list of all their backups--unless we duplicate the "can delete" logic in the BZI, which doesn't fit the BZI's purpose. BZIs must have an HTTP API Delete Backup can call. The BZI must know the backup job associated with a given backup because Delete Backup will care.
   -  Delete Backup cares about backup jobs for backups because data changes in the UI can affect a backup's delete date. So, it must get data for all backup jobs from the UI to calculate the delete date. Also, it must calculate the delete date for every backup every time it runs.
   -  If the BZI determines "can delete," it needs the same UI data Delete Backup needs. So, it must get current data from the UI that might affect the delete date (holds, changes in time to keep the backup, etc.) to decide if a backup can be deleted.
   -  Alternative: The UI calls the appropriate BZIs to notify them of changes that may affect delete date, but that means another fast-responding HTTP endpoint and the risks and communications complexity that brings.
-  Each BZI must manage its own master store of backups (duplicate logic). If we have more than one BZI, at least one BZI-specific master store will not have the same shape as the other(s). (Painful experience speaking.)

If the BC owns the master store of backups, we resolve those problems.

-  The UI doesn't need to know about the BZIs to show backups for a backup job. It can call BC for a list of backups.
-  The BZI doesn't need an HTTP endpoint for the UI to call, making it simpler, reducing code duplication (out of sync in different BZIs) and other risks.
-  The UI can call BC when it makes a change that may affect delete date and let BC update the delete date for affected backups. It doesn't need to know about BZIs and the BZI doesn't need to get data from the UI to decide which backups can be deleted.
-  BC can find candidates for delete by getting backups that meet delete date criteria with a simple query.
-  The BZI doesn't need to maintain delete dates or have logic to determine if a backup can be deleted.
-  The BZIs don't own the master store of backups, so we don't need to worry about different BZIs having different shaped stores.
-  The communication lines between components are simpler, making the system easier to reason about.
-  Process boundaries are simpler, making the system easier to reason about and reducing risks.

### What are the process boundaries you mention?

-  The UI owns managing and presenting data about what can be backed up (backup jobs) and presenting data about what is backed up (backups requestd from BC as needed).
-  Each BZI owns creating, deleting, and restoring physical backups on a specific technology.
-  BC owns accepting and tracking requests to create backups, deciding which backups can be deleted, and accepting and tracking requests to restore backups and delete restored copies of backups.

## Development Plan

I want to lean on concepts of domain driven design, clean architecture, and clean code.

I'll start with BC's Create Backup function. That means I need:

-  An entity, data transfer object, and repository for backup requests
-  An entity, data transfer object, and repository (read only) for backup jobs
-  An entity and data transfer object for BZI results
-  An entity, data transfer object, and repository for backups

And that means I need:

-  An Entity abstract class I can extend to make my entities
-  Possibly a Value Object abstract class -- DDD concept that I'll probably need, but not sure yet.
-  A general Result class that gives me a simple interface over ok, fail, and similar results between entities and between layers of my solution.

So, let's start with the three core classes and some better code organization to support this model.

Then I need to define what makes up each entity.

### Backup Request

-  Request data
   -  API version -- put it in the request so callers don't need to change their URL
   -  backup job id -- when users create backup jobs, they'll need the backup job id to use in their request
   -  backup source path -- where does the BZI get data?
   -  data as of date -- the caller should know if this is data for 2021-01-01 or 2021-01-08 and tell us; part of expected delete date calculation
-  Tracking data
   -  id -- request id (UUID) assigned by BC
   -  state code -- current state of the request
   -  received request time -- when did BC receive the request?
   -  responded to requester time -- when did BC send a response to the requester?
   -  sent to BZI time -- when did BC send the request to the BZI?
   -  sent to BZI name -- to which BZI did BC send the request?
   -  received BZI result time -- when did BZI receive the result from the BZI?
   -  recorded BZI result time -- when did BC finish recording BZI results (both writes and delete)?
   -  master backup store backup id -- the backup id assigned to the backup in the master backup store

Do I want to put the backup request id on the master backup store?

-  Probably because if someone asks about a request id after I purge the request tracking store (timing TBD, but won't be forever), I can see if the request resulted in a backup.
-  Or, I could put it on the request tracking store and lose that ability. Not sure which makes the most sense at the moment, so think about it and decide later.

Also, I'm not sure I want to write requests to the request store as soon as I receive them. It may be wiser to only write allowed requests. That lets me respond to the requester with a failure code if needed. **BUT**, that's an HTTP assumption. If I'm using a queue to get requests, I won't respond to the requester because the queue won't let me. In that case, I'll need more detail.

So, let's replace "responded to requester time" with

-  checked allow time -- when did BC finish checking if the request is allowed?
-  responded to request time -- when did BC finish initial receipt processing for the request?

That set of attributes gives us deep insighed into request handling performance.

And add

-  request transport -- which transport provided the request (HTTP, queue, etc.)

The "request transport" attribute lets us:

-  Understand which transports are being used most and identify possible differences or issues with specific transports (may drive support decisions)
-  Choose to handle requests differently if needed.

Why would we handle requests differently for different transports?

-  For HTTP, BC should check for allowed before responding so they know if the request failed
   -  If the request wasn't allowed, the requester has a problem in their process they need to fix.
   -  If the UI didn't respond, the requester should retry after a short delay.
-  For a queue sourced request, consider the failure paths and how they can create unwanted coupling.
   -  If the allow check fails because the request isn't allowed, BC should delete the request from the queue and notify (email) someone a request wasn't allowed.
   -  If the allow check fails because the UI isn't responding, BC can't tell the requester it wasn't their fault and to retry.
   -  Tying the queue handling to the allowed check creates coupling between the queue reader and the allowed check. That coupling means the queue reader needs to be on the same circuit breaker as the allowed check, even though they're calling different services OR it means the queue reader will **re-poll the same message from the queue possibly many times** if the the UI doesn't respond.
   -  If BC's queue reader writes received request before the allowed check and deletes the request from the queue, BC can retry unchecked requests when the UI circuit breaker closes. If BC fails or is restarted while waiting on the UI, it can read unchecked requests and continue with them.

We've removed the coupling between the business logic (checking if allowed) and the transport (queue).

We're starting with the HTTP transport, not the queue transport, but this line of thinking tells us the Backup Request entity needs to support the allowed check and likely persistence separate from request creation.

## Next steps

-  Build the core classes for Entity, Result, others TBD.
-  Begin the Backup Request entity--properties interface (DTO), private constructor, create, others TBD.
-  Begin the Backup Job entity--properties, private constructor, create, others TBD.
-  Build repos for both entities.
-  Build use cases for both entities.
-  Build controller to receive requests.
-  Wire into an Express API.

AND WRITE TESTS FOR EVERYTHING AS YOU GO!!!

## References

Khalil Stemmler's' series of posts on [Domain Driven Design with TypeScript](https://khalilstemmler.com/articles/categories/domain-driven-design) give decent summaries of key concepts and code examples. I may not follow his lead 100%, but I plan to understand what he's doing and use what makes sense to me. His [white-label](https://github.com/stemmlerjs/white-label) repo on GitHub has a complete application demonstrating how to put it all together, though it may take a while to figure out his project organization choices. (Not saying they're bad, just that I'm still figuring out what's where.)

Eric Elliot has a series of posts on Medium that give a decent grounding in functional JavaScript (and therefore TypeScript) concepts. Stemmler seems to agree with most of Elliot's key points, though they differ in how they do it. For example, both favor factory functions to create objects, but Elliot seems to lean toward no `class` where Stemmler loves `class` and puts the factory function in the class with a private constructor. Both approaches achieve the same end. Elliot walks through key functional programming concepts, which Stemmler doesn't.

**COMMIT: define the problem and design**
