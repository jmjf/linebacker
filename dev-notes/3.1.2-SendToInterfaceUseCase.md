# Get send to interface use case working

## Planning and testing notes
This use case needs a backupRequest. It is separate from the other use cases because the startup recovery process will look for any allowed but not sent requests and send them.

The use case may be called by:
* An event triggered by the check request allowed use case
* A startup recovery process that has a list of backup requests from the repo. (??? let's say for now, but maybe recovery triggers events)

I don't need an actual repo yet because the use case will always get data from something else (event from check request allowed or startup recovery process).

While this is the third step in the process, I'm writing it second because the second step (check request alllowed) needs to trigger it from an event.

### Handling events
In DDD, events happen in the domain and are managed on aggregates, not entities. So, I'll need to create an `Aggregate` base class and change `BackupRequest` to an aggregate.

In DDD, events may call other subdomains (bounded contexts) within the domain or may call the same subdomain. If we think of a feature like "create-backup" as a subdomain, the event is observed by the same subdomain.

I'm probably abusing the term "subdomain" by comparing it to a feature. Maybe the name "create-backup" is wrong. Maybe it's the backup request subdomain. At first glance, that idea seems like it might be right. I'll think about it more and consider renaming.

Questions to consider:
* Is "recover on start" a use case? (maybe)
* Does it belong in a separate feature (subdomain???) within `linebacker`? (maybe)

Thinking about those questions, if we see application startup as something external that happens, we could have a startup controller that runs when the application starts. It would call use cases to recover different features/subdomains. In that case, each feature/subdomain should provide a recover use case that knows how to recover itself. That idea seems sound at first glance--putting responsibility for recovery in the subdomain instead of making some central piece understand how to orchestrate it across everything.

### Is "startup" an event?
* Subdomains subscribe to events. Stemmler uses `modules/[subdomain]/subscriptions`, which defines each event handler (`IHandle`) and `.../subscriptions/index.ts`, which creates the handler with an instance of any use cases it needs.
  * Each `index.ts` is imported in `src/index.ts`, which also imports `shared/infra/http/app` and `shared/infra/database/sequelize`. The code is relying on the fact that importing a file (`index.ts` in all these) runs the code in it, and that the code in each `index.ts` is doing setup. So, `src/index.ts` is the start of execution.
* The base aggregate class includes the aggregate's event logic. I'm not clear on how Stemmler's code is working. He defines the DomainEvent class, but I haven't found where he creates an instance.
  * In `modules/[modulename]/domain/events` he defines individual events that implement `IDomainEvent`.
  * In `.../domain/[aggregateName].ts` he calls `[aggregate].addDomainEvent(new [event]([aggregate])`).
  * In `AggregateRoot.ts`, `addDomainEvent()` adds it to the aggregate's `IDomainEvent[]` array and calls `DomainEvents.markAggregateForDispatch(this)`.
  * In `events/DomainEvents.ts`, `markAggregateForDispatch(aggregate)` pushes the aggregate onto an `AggregateRoot[]`.
  * In the same class, `dispatchEventsForAggregate(id)` is called by a Sequelize hook, where the hooks tie into each Sequelize model. So, this relies on an underlying feature of Sequelize to trigger actions after database changes happen.
    * Which makes a kind of sense because the create use case creates an instance of the aggregate then passes it to the repo to save. So creating the aggregate sets up the event to be dispatched before the database action happens.
  * The key is that everything in `DomainEvents` is static. It exists without being instantiated, so the class is an application-wide singleton.

Back to the question, is startup an event?

`subscriptions` hints that it could be. Event subscriptions are created as part of application startup, so I could create subscriptions to startup in each subdomain that run a startup use case and fire a startup event.

BUT:
* What aggregate would that event belong to?
* The event isn't triggered by a database hook, so I'd need to trigger it specifically.
* It only happens once per application execution, where most domain events happen many times.
* Startup needs to run events in a particular order (not guaranteed by events).

So I think it might be easier to give every subdomain a startup use case and controller.
* On application startup, before opening inbound communication, run each subdomain's startup controller.
* Startup controllers run the startup use cases.
* Startup use cases get stuck requests from the repo and trigger events in reverse order
  * In linebacker's case, trigger send events, then trigger check events.
  * Why? If check runs and triggers send events before I start looking for stuck requests ready to send, I could double-send a request. While I hope the backup store interface is idempotent, I don't want to send requests I don't need to send.
* When all startup use cases finish, open inbound communication.

Each state's startup use case needs to be separate. When I have a circuit breaker for the connection to the backup store interface or the service that gives me data about the backup so I can confirm it's allowed, if those connections fail, I could end up with events backed up that I need to recover when the connections are restored. BUT a better solution might be to improve `DomainEvent`'s error handling so it doesn't throw away events if dispatch fails. Stemmler's events return `Promise<void>`, so would need to return something that lets the event dispatch.

I don't need to solve this problem now, but need to be aware it exists so I can think about how to solve it.
* I could build a queue that marks events as in process and recovers them if they aren't handled, but that seems complex.
* I don't want to have failure push a new event into the aggregate's list of events because that could put me in an infinite loop of event handling, I think.
* I'm not sure how I signal to the event dispatcher that the events can't process because communication is down, so don't bother to try to dispatch.
* Maybe this isn't an issue because none of the event dispatching code waits for events to complete, so if an event fails and gets requeued, that should happen after the dispatch cycle finishes. This may be the key if the handler code does something async, which would be the only real risk point, I think.

There's a lot to think about here, but again not something I need to solve right now, just be aware exists. For now, I'll make use cases per subdomain that deal with startup/recovery and figure out how to wire them in later. Because I'm confident I want those to be use cases so the logic stays in the subdomain.

### Side note: idempotent backup store interface requests
I think the backup store interface should get information that drives how it names the store--destination base name, data as of date, data generated date--and report duplicates in status messages. That would let it be idempotent if for some reason a requester sent two requests for the same data or something caused BC to send the same request twice. BC can use the status to note the request is for an instance that already exists. I want the requester to tell me everything that gets filled in on the base name.

### Controllers to routes
I spent a while figuring out how Stemmler handles general startup.

* Each use case in `useCases` has `index.ts`, which imports the use case, controller, and repo instances needed and:
  * creates the use case with the repo(s)
  * creates the controller with the use case
  * exports the instances of use case and controller
  * repo instances come from a subdomain level `repos/index.ts`, ensuring only one of each repo exists
* Each subdomain (Stemmler => "module") has `infra/http/routes/index.ts` that imports the instances of the controllers and ties them to a router instance (`express.Router()` in Stemmler's case).
* In `shared/infra/http/api/v1.ts`, import the routers for each subdomain, create a router (`v1Router`) and bind any non-domain routes (application health check) and the individual routers to it.
* In `shared/infra/http/app.ts` import `v1Router` from `api/v1.ts` and `app.use('/api/v1', v1Router);`

This approach makes sense because:
* The domain layer (entity, aggregate, value object, etc.) knows nothing about anything outside it, as it should be.
* Each use case's `index.ts` knows how to create the controllers, repos, use cases, etc., for that use case, keeping use case setup in the application layer.
* Each subdomain's infrastructure layer knows how to bind controllers to routes for that subdomain, keeping subdomain route knowledge in the subdomain.
* The top level `app.ts` knows how to bind each subdomain's routes to the top level router, keeping top level infrastructure route knowledge in the top level infrastructure layer.

If I create a v2 API, I can create a `v2Router` and bind it to the top level router, reusing v1 routes that didn't change. I'm guessing I might have subdomain level routers with different versions as needed. I plan to handle versions in the DTO with an ApiVersion member, so need to think about how that plays out.

## First steps
[x] Create `create-backup/use-cases`
[x] Start request backup use case in `create-backup/use-cases/send-request-to-interface`
[x] Create a DTO the same place.
[x] Stub out use case so it returns a `left()` result.
[x] Write test for use case -- when executed, the result isRight() (use case will fail)
[x] Confirm test fails
[x] Change use case to return a right() and confirm test passes.

A `use-cases` subdirectory in `create-backup` makes more sense to me now. I'm almost certain `create-backup` will become `backup-request`, but can wait until I'm sure.

I don't see this use case running from anything other than an event for now. In the future, maybe I'll have an application control endpoint that lets me run it, I can wait to build the controller until then, because the event handler will pass an id so it can find the request it needs to send.

For now, DTO has a `requestId` only.

I'm assuming the use case will need a repo to get the request. But I could, in theory, pass it the whole backup request because all my uses for this use case can get a backup request. But normal pattern is to get it from the repo, which also ensures that, if anything else mutates the data in persistence, this use case gets the latest data.

**COMMIT: 3.1.2.1 - stub out use case and first test**

## Add use case logic
[x] Get request from repository (returns a `BackupRequest`)
[x] Send message to backup store interface (adapter call, so need an adapter)
[x] Set request status, status timestamp, etc.
[x] Update request in repo

[x] `BackupRequest` - Set status to sent
[x] `BackupRequestRepo` (interface and test stub) - rename `getRequestByRequestId` to `getById`
[x] `BackupRequestBackupInterfaceAdapater` method `sendMessage(BackupRequest)`

[x] test: when everything is okay, returns success and message is sent (status)
[x] test: when request is already sent, returns success and message is sent (status)
[x] test: when request doesn't exist, returns failure
[x] test: when status is NotAllowed, returns failure and message status is unchanged
[x] test: when send message fails, returns failure and message status is unchanged

Found issues in the `test-utils/*Factory.ts` functions. Changed to return `Promise.resolve()`. They should work now.

**COMMIT: 3.1.2.2 - add use case logic**

## Other enhancements
[x] Change `CreateRequestUseCase.spec.ts` to use the repo factory method instead of manually mocking

Replace empty object repo with factory call.
Replace `repo.save = jest.fn().mockResolvedValue()` with `const saveSpy = jest.spyOn(repo, 'save')` and check `saveSpy` has been called or not.

This was easier than expected. Cool.

**COMMIT: 3.1.2.3 - use repo factory in create request tests**

[] Add `toString()` to base Entity class to make it easier to log objects

**COMMIT: 3.1.2.4 - add toString() to Entity**

## When will this use case be called?
* From an "request is allowed" event when a request is checked for allow
* From startup if requests are found that need to be sent
* From a circuit breaker when the interface changes from unavailable to available

Startup: will have a list of requests that it pulls from the DB
Circuit breaker: will have a list of requests that were waiting (if stopped before circuit breaker closes, startup will recover us)

Both those cases have request data they could send to the use case so it didn't need to read the database.
Could anything change the data between their copy and the use case running? (Startup: no; CB: should be no b/c can't get a reply until interface gets it)

Event:
* Check for allow has a request that it saves, so has data it could send
* Could anything change that request's data? Not that I can think of.

So, I think I could pass the request data to the use case so it doesn't need to call the DB to get it.
But do I want to do that? It uses cached data, though data is cached for a short time and I can't think of anything that would change the request data.
OTOH, is it really a huge hit to call the DB? Sure it takes time, but we're going for a single row by a PK index. It also adds some insurance if the save fails (row won't exist).

That last point is probably the most important reason to keep this pattern and leads to another enhancement I need to make.

[] Change `test-utils` factories' `getById()` methods to throw if the `getByIdResult` is undefined or null. (Aligns with how the real repo will behave.)
[] Remove check for backup request undefined or null in use case.

**COMMIT: 3.1.2.5 - test repo/adapter factories throw if no `getByIdResult`**