# Rethinking scaling, recovery and interprocess communication that requires

## Issues, concerns, motivations

Thinking about how this would run in a production environment, where we want to ensure uptime, and considering the possibility of running more than once instance for scalability or uptime assurance, I've identified some things that need some thought.

-  In production, I'll probably run linebacker with pm2
-  For uptime assurance, I may run more than one instance of linebacker
-  There's no guarantee instances will be on the same server, so interprocess communication (IPC) with native mechanisms may not work
   -  Based on what I've read, if I run instances on different servers, they won't be able to us Node's built-in messaging or PM2's IPC mechanisms because those assume a single server scenario
-  Less likely, but notable, is that the existing solution could have scalability challenges if one part of the process is a bottleneck; scaling is all or nothing
-  Currently, events to retry are stored in memory; if the application crashes it needs to recover the lost events on startup
-  Startup recovery, as it is today, does not provide assurances that two instances of linebacker won't process the same requests at the same time, causing duplicate requests
   -  We have no guarantee that the backup interface component will handle duplicates sanely

So, I'm looking at questions like:

-  How do I run more than one instance safely
-  If I break down the use cases into separate services, how do I handle IPC
-  How do I ensure IPC works cross-server
-  How do I ensure startup recovery doesn't create duplicate messages

## Thinking it through

If linebacker accepts a request (returns a 202 to the caller), the request exists in persistent storage, so is available to recover. The issue is about ensuring that the remaining use cases in the workflow know they need to process the request and ensuring that only one instance of the application processes the request.

`DomainEventBus` is a pub/sub type mechanism. Subscribers register with the bus and it publishes events to all subscribers. If a subscriber isn't subscribed, it misses any messages. In a solution that separates the subscriber use cases into separate processes, I need either guaranteed delivery and processing (temporary invisibility concept similar to what Azure Storage queue uses) or I need a way to designate that one instance of the process is responsible for startup recovery.

Startup recovery presents some challenges. Assume:

-  linebacker is running with two "CheckAllowed" instances
-  One process is designated as the instance to run startup recovery
-  The "recovery" instance restarts
-  While the instance is restarting, a request arrives
-  The second process picks up the request and begins processing it
-  Startup recovery reads "Received" requests before the second process finishes processing

What if I say:

-  The queue guarantees at-least-once delivery
-  The queue delivers to one and only one subscriber per event visibility window
   -  When read, the event is invisible to other subscribers until invisibility expires
   -  This feature should ensure that the processor either completes the event or has failed
-  The "recovery" instance publishes "received" events to the queue
-  The "recovery" instance doesn't start listening to the queue until it has published all recoverable messages

In that case, the conflict is unlikely because the "recovery" instance wouldn't process any events based on the data it reads from the database. If the secondary instance reads the event, the "recovery" instance won't see it until the secondary has had plenty of time to process it or fail. For extra assurance, I may want the "save" method in database adapters to include an optional status value or other check to ensure the request is in the expected status. In that case, I'd need a way to ignore status for new requests or to identify duplicates in an unsuccessful status (not allowed, failed). Allow new on unsuccessful to support callers submitting the same request as a retry.

Would I really need to read data from the database to decide what to retry? Or is the idea that the queue will restore a message that isn't deleted good enough? I think it depends on the queue and where it keeps data. Assume the whole infrastructure fails and the queue solution restarts. If the data is persistent, then no "read the database" recovery is needed. If the data is in memory, then restart does need to happen.

-  But, consider, recovery could be a process that runs when the queue restarts, which further reduces the risk of duplicate messages

All these ideas assume some kind of message queue or similar IPC solution. So, what are my options?

-  Kafka
-  RabbitMQ
-  BullMQ
-  Redis-smq

Kafka is not trivial to set up. It requires Zookeeper and seems a bit hefty. It does everything, but may be overkill.

RabbitMQ requires the broker to be installed. It runs on Lin/Mac/Win. It does require disk space to work against, so if the environment has restrictions on applications writing to the server's disk, could run into issues. But there is no solution here that doesn't seem to require disk, so... The RMQ examples on their site use callbacks, but a Promise API is available. RMQ uses AMQP and, for Node, `amqplib`, which supports 0.9.1 (not 1.0).

BullMQ runs on top of Redis, which means it works for Lin/Mac (Redis does not run on Windows without WSL2, which means it's running under Linux). BullMQ is a job queue, which fits my use case pretty well, but that leads to a lot of job specific language. I'd need to invest some time digging into examples to understand how to use it with linebacker. (I think it could work, but needs some time invested in examples. AMQP and Kafka are easier fits.)

Redis-smq runs on top of Redis, so Lin/Mac only. Redis-smq queues are LIFO. I don't think that's a deal breaker, but I'll need to think about how that might affect assumptions. It would need some investment in reading the docs and putting scattered pieces together.

## Translating BullMQ

After some more reading, BullMQ seems like a decent fit. It translates as follows.

| BullMQ | Existing       |
| ------ | -------------- |
| Queue  | Event name     |
| Worker | Subscriber     |
| Job    | Event instance |

That comparison leads me to the following outline for using BullMQ.

-  HTTP request receiver
   -  Receive HTTP requests (reject invalid requests)
   -  Write request to the database
   -  Publish job to the Created queue
   -  Job id = request id
   -  Generally, this is the HTTP part of the application today with some adjustments to send to BullMQ
-  CheckAllowed worker service
   -  Creates a worker that listens to the Created queue
   -  When a job is received, execute the use case
      -  Use case save publishes allowed status jobs to Allowed queue
      -  Option: NotAllowed queue with a new worker/use case to notify requesters that a request was not allowed
      -  NotAllowed and not in Received status -> treat as completed; bad data -> failed; connection error -> retry
      -  Retry delay = moderate (5 minutes or more)
   -  Generally, this is a new mini-app that includes the CheckAllowed-related subscriber, use case, etc.
-  SendToInterface worker service
   -  Creates a worker that listens to the Allowed queue
   -  When a job is received, execute the use case
      -  Option: Sent queue with a new worker/use case to notify requesters that a request was sent
      -  Not in Allowed status -> completed; bad data -> failed; connection error -> retry
      -  Retry delay = long (10 minutes or more)
   -  Generally, this is a new mini-app that includes the SendToInterface-related subscriber, use case, etc.
-  ReceiveStatus worker service
   -  The external queue listener publishes jobs to the StatusReceived queue and deletes messages from the external queue
   -  When a job is received, execute the use case
      -  Option: StatusUpdated queue with a new worker/use case to notify requesters that a request completed/failed
      -  Bad status -> completed; bad data -> failed if retries - connection retries > threshold; conection error -> retry
      -  Retry delay = moderate (5 minutes or more)
   -  Generally, this is a new mini-app that includes the ReceiveStatus-related subscriber, use case, etc.

Use PM2 to run each of the above sub-services as separate processes. Scale independently as needed.

### Points to consider

BullMQ requires Redis to be available. If it isn't, BullMQ can't add messages to the queue. The other queuing solutions have similar issues (if Kafka isn't available, can't publish to Kafka; if RabbitMQ broker is down, can't publish to RabbitMQ; redis-smq requires Redis). How do I ensure reliable queue updates?

-  Option: Store data on the queue only, not on the database
   -  `Queue.getJob(jobId)` gets a specific job; if jobId = request id, I can query the queues for the job to find its status
   -  `Queue.getJobs(...)` gets jobs that are not in BullMQ completed, failed, or delayed; can list all pending jobs
   -  `Queue.getCompleted()`, `Queue.getFailed()`, and `Queue.getDelayed()` gets jobs in those BullMQ statuses
-  Option: In the use case, if the request status is a status the use case would set, do nothing and return ok
   -  Before adding a job to the "next" queue, check if a job already exists for the jobId and don't add if already there (prevent duplicates)
   -  See note below about stalled jobs
   -  This option requires startup recovery for Received status only (to ensure they make it into the Created queue)
      -  When an instance of the HTTP receiver starts, get start time
      -  Wait a few seconds to give other instances a chance to queue any in-flight requests
      -  Get any Received requests before start time
      -  If request does not exist on Created queue, add it

According to Workers | Stalled Jobs, if a worker doesn't notify status within the `stalledInterval` (default 30,000 ms), the job will move back to BullMQ `wait` status and be retried up to `maxStalledCount` (default 1) times

-  Interval may need to be longer based on async timeouts in other steps
-  Count: Under what conditions would we fail a job due to worker failures

BullMQ's retry model has a fixed number of retries per job. Retries happen when a worker processor throws an exception.

-  Be cautious about throwing exceptions from worker processors; use BullMQ's native retry rarely
-  Prefer to explicitly move jobs to failed status
-  If a failure requires retry, use `Job.moveToDelayed()` to drive the retry

Consider hooking into `QueueEvents` for logging and other common activities that happen when jobs are completed, failed, delayed, etc.

Consider how we might keep/get statistics and other information about queues to support observability. For example: `Queue.getJobCounts()`, `Queue.getCompleted()`, `Queue.getFailed()`, etc. Consider how settings governing how much completed/failed data to keep affect observability. (This may be a non-issue with good logging--use the logs instead.)

## What needs to change

-  Need apps for each sub-service
   -  HTTP receiver
      -  Express server components
      -  Express controller
      -  Request repo
      -  Create request use case
   -  Check allowed service
      -  Worker based on subscriber
      -  Request repo
      -  Backup job service
      -  Check allowed use case
   -  Send to interface service
      -  Worker based on subscriber
      -  Request repo
      -  Backup interface adapter
      -  Send use case
   -  Receive status
      -  Worker based on subscriber
      -  Request repo
      -  Backup repo
      -  Backup job service
      -  Receive use case
   -  Backup interface receiver (new)
      -  Receive from external queue
      -  Send job to Receive Status queue
      -  Delete from external queue
-  How would I manage zpages healthz statistics
   -  Should be part of HTTP receiver (because that's where readiness matters to callers)
   -  Should get stats from other jobs where possible
      -  What can I get from PM2
      -  What can I get from BullMQ

Build the backup interface receiver and receive status pieces first.

-  Not integrated into the main application, so not tearing the app apart (probably should be separate even without MQ)
-  Include both producer and consumer sides, so can sort out the key pieces
-  Avoid restart recovery considerations
-  Sort out logging issues
-  Experiment with PM2 and scaling
-  Can get early feedback fairly quickly (don't need to decompose the whole application yet)

Goals

-  Get receive status working with BullMQ; sort out how it affects infrastructure
   -  Request status moving to failed/succeeded trigger a notification event (how to interface logical events to physical messages)
   -  Notification event gets sent to notification queue; just log it for now
-  Build a generic interface that can work with other messaging solutions
   -  Can I hide BullMQ specifics behind the interface and, if a MQ doesn't need the function, just stub the function to do nothing
   -  Likewise, if a MQ requires something BullMQ doesn't, stub the function for BullMQ
-  Use PM2 to run the main app and the status receiver components
-  Update healthz with PM2 and BullMQ stats

After working through the issues

-  Pull send to interface into a separate sub-service
-  Pull check allowed into a separate sub-service
   -  Affects startup recovery because it no longer needs to check for Allowed requests
