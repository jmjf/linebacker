# Separate backup interface receiver

## Setup

BullMQ requires Redis, so I need to add Redis to my Docker configuration. The latest Redis images on DockerHub are 6.2.4. Redis docs recommend using redis-stack in development, which includes both Redis and RedisInsight (visibilty tool).

I was trying to set up a configuration and ACL list on Redis, but having trouble getting the config file to mount, so I'm leaving it for now.

I'm moving dev to a Linux machine and relocated the Splunk volumes in `.gitignore`d directories in `docker-compose`, but that brought in some `test.js` files (don't know why they'd include that), which confused Jest. Added `roots` key in `jest.config.js` to test only `src`.

My Splunk transport wasn't working, then I realized I needed to pipe to `pstBin`, not `pinoSplunkTransport`. Updated md file 15.1 so I can find it in the future.

So, the command line is `npm run express:dev:typeorm | APP_ENV=dev npx ts-node src/infrastructure/logging/pstBin.ts`.

But I'm getting an error from `SplunkLogger`. In `flushQueue`, when called from the timeout, `this._inQueue` is undefined. I changed the timeout to run `this.flushQueue.bind(this)`, though I'm not sure why it was losing context.

Splunk is working now.

Testing a request, I see that if the queue doesn't exist, I get an unreadably massive error. Added TODO item to reduce the error size. Meanwhile, created the queue. Restarting runs startup recovery, which posts the request to the queue. And `npx ts-node localQueue.ts read` shows the message.

So, I think my dev environment is working now.

**COMMIT: CHORE: get dev environment working; add Redis to docker**

## Interface communication channel to BullMQ

I don't have a component to receive data from the backup interface and relay it to the use case. Most of the parts exist, but I don't have the polling loop in written. I do have an example of a polling loop from previous work. I'll use that as a basis, but use the queue adapter for backup requests.

-  While not halt (set halt on `AbortController.signal`)
   -  Read from Azure Queue using the adapter
   -  If messages found, call message handler for each message
   -  If handler succeeds, delete message
   -  If handler fails and not a connect error and too many dequeues, send to error message handler (write to log, delete)
   -  If no messages found, delay (abortable, on abort, set halt)
-  Message handler
   -  `JSON.parse()` the message text
   -  If parse fails, error
   -  Assemble data for BullMQ job
   -  Add to BullMQ queue
-  Error handler
   -  Write message to log
   -  Delete message from queue

### What can fail?

-  Azure read can fail; I think I can ignore this and just delay
-  Azure delete can fail; more problematic because it can cause duplicates
   -  How can I track messages that failed to delete and ensure they're deleted; maybe check for them on "next" queues if I have enough history
-  Need to ensure that, if a Redis connect error failure we don't delete even if dequeue count is over limit
-  Need to ensure the loop halts on signal
-  `JSON.parse()` can fail; ensure error is handled correctly
-  Add to queue can fail; ensure it returns a connect error

What tests do I want for the queue watcher?

-  when start is called, the queue watcher moves to started status and the loop runs
   -  Check if loop runs by spying on `readMessages` call; expect to be called
-  when `AbortController` signals, the queue watcher loop stops and the queue watcher is in halted status
   -  Pass loop delay to the queue watcher; use a short delay, reset mock, halt, delay long enough for queue watcher to loop 2x, expect `readMessages` not to be called
-  when halt is called, the queue watcher is in halted status and the loop stops

## Work notes

Moved Azure Queue components to `src/infrastructure/azure-queue`, will build queue poller there. If I used a different backup interface infrastructure, it can go in a separate directory.

Moved queue read and delete methods out of `IBackupInterfaceStoreAdapter` to `IAzureQueueAdapter`

-  The queue watcher only cares about read and delete, IAQA lets us pass any read + delete implementer (for delete backup, restore backup, etc., in future)
-  `AzureBackupInterfaceStoreAdapter` implements both IBISA and IAQA, so can serve either role
-  I left implementations in ABISA because they need the circuit breaker, among other things, and I don't want to think through how to pass those around right now (maybe later)
-  Could I move send to IAQA and have ABISA call it from a function that maps for store data and calls IAQA function

Built tests and got the passing to prove loop control is working.

Added a test to confirm the message handler runs. Call message handler and prove test passes. Don't await message handler call because awaiting could delay event publish, which could let `popReceipt` expire before the subscriber has a chance to process the message.

**COMMIT: FEAT: add queue watcher; get basic functions working**

Built the server/app/init code to run the queue watcher and hook everything up so it should work. I'm getting errors reading the queue. I decided to try to use the debugger.

After some misleading search results, I found that I can run apps in the debugger with a script from `package.json`. In the Debugger panel, in the dropdown for the green arrow at the top of the panel, select `Node.js...`, then choose the script to run. Be sure to set a breakpoint before starting. Beware of breakpoints on HTTP calling functions because they'll timeout and fail. Set the breakpoint immediately after the HTTP calling function and examine results.

I made some Q&D changes to xBISA to deal with type conflicts while trying to get the debugger to run, will clean up later (added "NEXT" list below).

Not seeing queue HTTP errors anymore and the loop seems to be running happily.

I added a "send" option to `localQueue` and sent a success status message for a backup request in the table as Sent. I didn't set a breakpoint on the subscriber in time, so didn't see it run, but the request is updated and an instance created with data that matches the message. So, it works.

Changed `messageType` to `queueName` in `AzureQueueWatcher`. It makes more sense to log the queue name than a made up "message type".

**COMMIT: FEAT: build queue watcher runner code; see it work**

Built a queue helper API that can:

-  List all queues (`GET /queues`)
-  Get queue properties (`GET /queues/:queueName`)
-  Create a queue if it doesn't exist (`PUT /queues/:queueName`)
-  Delete a queue (`DELETE /queues/:queueName`)
-  Get messages in a queue (`GET /messages?queueName=:queueName`; be aware of visibility timeout and how it may affect results)
   -  parsedMessageText is `JSON.parse(messageText)` if parseable, {} if not parseable
-  Post a message to the queue (`POST /messages` with body `{ queueName: string, message: object | string }`)
-  Delete a message from the queue (`DELETE /messages` with body `{ queueName: string, messageId: string, popReceipt: string}`)
   -  The body must contain those three values but can contain other (ignored) values
   -  Easy use: get messages, then copy messageId and popReceipt with any data between them and paste into body

The API runs on env API_PORT + 5.

Exported Insomnia config to dev_notes; note that exported data assumes port 3005. Adjust if needed.

**COMMIT: FEAT: build queue helper API to make working with Azure queues easier**

NEXT

-  need a better `getNextDelay()` method; decide how you want to manage delays
-  give receiver it's own port in `dev.env`
-  in `initQueueWatcher`
   -  pass a delay that will set the start delay for the queue watcher
   -  move the start log into `AzureQueueWatcher` so the app doesn't need a queue name; also halt log
-  go through [I, Azure, Mock]BISA adapters and ensure return types make sense; clean up as needed
-  think through how you really want to implement queue adapters (like it is; generic code; specific adapter wraps parent as needed; etc.)
-  rename `expressServerTypeorm` to `expressApiSrvTypeorm` (etc.) for consistent naming
-  consider adding `getQueueName` to the adapter so I don't need to pass queue name to the queue watcher, can just get it from the adapter in the constructor

## TODO list

-  Improve Azure Queue adapter or subscriber SDK error handling; choose what makes sense to include in Splunk logs to avoid huge errors
   -  Selecting parts of `request` should go a long way toward improving matters
      -  keep `url`, `method`, `body`, `requestId`
      -  but maybe start by removing `request.operationSpec` and seeing what else might be in `request` that's worth keeping
-  Figure out Redis configuration so I can secure Redis (simulate real world production)
-  When application `AbortController` fires, delay long enough for things to shut down
   -  Also need to trigger abort on SIGINT or similar
-  In `AzureQueue.receiveMessages()`, make the visibility timeout configurable
   -  From environment or from caller as a parameter (TBD)
