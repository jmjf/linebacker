# Separate backup interface receiver

## Setup

BullMQ requires Redis, so I need to add Redis to my Docker configuration. The latest Redis images on DockerHub are 6.2.4. Redis docs recommend using redis-stack in development, which includes both Redis and RedisInsight (visibilty tool).

I was trying to set up a configuration and ACL list on Redis, but having trouble getting the config file to mount, so I'm leaving it for now.

I'm moving dev to a Linux machine and relocated the Splunk volumes in `.gitignore`d directories in `docker-compose`, but that brought in some `test.js` files (don't know why they'd include that), which confused Jest. Added `roots` key in `jest.config.js` to test only `src`.

My Splunk transport wasn't working, then I realized I needed to pipe to `pstBin`, not `pinoSplunkTransport`. Updated md file 15.1 so I can find it in the future.

So, the command line is `npm run express:dev:typeorm | APP_ENV=dev npx ts-node src/infrastructure/logging/pstBin.ts`.

But I'm getting an error from `SplunkLogger`. In `flushQueue`, when called from the timeout, `this._inQueue` is undefined. I changed the timeout to run `this.flushQueue.bind(this)`, though I'm not sure why it was losing context.

Splunk is working now.

Testing a request, I see that if the queue doesn't exist, I get an unreadably massive error. Added TODO item to reduce the error size. Meanwhile, created the queue. Restarting runs startup recovery, which posts the request to the queue. And `npx ts-node localQueue.ts read` shows the message.

So, I think my dev environment is working now.

**COMMIT: CHORE: get dev environment working; add Redis to docker**

## Interface communication channel to BullMQ

I don't have a component to receive data from the backup interface and relay it to the use case. Most of the parts exist, but I don't have the polling loop in written. I do have an example of a polling loop from previous work. I'll use that as a basis, but use the queue adapter for backup requests.

-  While not halt (set halt on `AbortController.signal`)
   -  Read from Azure Queue using the adapter
   -  If messages found, call message handler for each message
   -  If handler succeeds, delete message
   -  If handler fails and not a connect error and too many dequeues, send to error message handler (write to log, delete)
   -  If no messages found, delay (abortable, on abort, set halt)
-  Message handler
   -  `JSON.parse()` the message text
   -  If parse fails, error
   -  Assemble data for BullMQ job
   -  Add to BullMQ queue
-  Error handler
   -  Write message to log
   -  Delete message from queue

### What can fail?

-  Azure read can fail; I think I can ignore this and just delay
-  Azure delete can fail; more problematic because it can cause duplicates
   -  How can I track messages that failed to delete and ensure they're deleted; maybe check for them on "next" queues if I have enough history
-  Need to ensure that, if a Redis connect error failure we don't delete even if dequeue count is over limit
-  Need to ensure the loop halts on signal
-  `JSON.parse()` can fail; ensure error is handled correctly
-  Add to queue can fail; ensure it returns a connect error

What tests do I want for the queue watcher?

-  when start is called, the queue watcher moves to started status and the loop runs
   -  Check if loop runs by spying on `readMessages` call; expect to be called
-  when `AbortController` signals, the queue watcher loop stops and the queue watcher is in halted status
   -  Pass loop delay to the queue watcher; use a short delay, reset mock, halt, delay long enough for queue watcher to loop 2x, expect `readMessages` not to be called
-  when halt is called, the queue watcher is in halted status and the loop stops

## Work notes

Moved Azure Queue components to `src/infrastructure/azure-queue`, will build queue poller there. If I used a different backup interface infrastructure, it can go in a separate directory.

Moved queue read and delete methods out of `IBackupInterfaceStoreAdapter` to `IAzureQueueAdapter`

-  The queue watcher only cares about read and delete, IAQA lets us pass any read + delete implementer (for delete backup, restore backup, etc., in future)
-  `AzureBackupInterfaceStoreAdapter` implements both IBISA and IAQA, so can serve either role
-  I left implementations in ABISA because they need the circuit breaker, among other things, and I don't want to think through how to pass those around right now (maybe later)
-  Could I move send to IAQA and have ABISA call it from a function that maps for store data and calls IAQA function

Built tests and got the passing to prove loop control is working.

Added a test to confirm the message handler runs. Call message handler and prove test passes. Don't await message handler call because awaiting could delay event publish, which could let `popReceipt` expire before the subscriber has a chance to process the message.

NEXT: need to write a better `getNextDelay()` method; decide how you want to manage delays.

## TODO list

-  Improve Azure Queue adapter or subscriber SDK error handling; choose what makes sense to include in Splunk logs to avoid huge errors
   -  Excluding `request` should go a long way toward improving matters
-  Figure out Redis configuration so I can secure Redis (simulate real world production)
-  When application `AbortController` fires, delay long enough for things to shut down
   -  Also need to trigger abort on SIGINT or similar
-  In `AzureQueue.receiveMessages()`, make the visibility timeout configurable
   -  From environment or from caller as a parameter (TBD)
