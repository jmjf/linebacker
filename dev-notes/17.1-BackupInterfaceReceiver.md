# Separate backup interface receiver

## Setup

BullMQ requires Redis, so I need to add Redis to my Docker configuration. The latest Redis images on DockerHub are 6.2.4. Redis docs recommend using redis-stack in development, which includes both Redis and RedisInsight (visibilty tool).

I was trying to set up a configuration and ACL list on Redis, but having trouble getting the config file to mount, so I'm leaving it for now.

I'm moving dev to a Linux machine and relocated the Splunk volumes in `.gitignore`d directories in `docker-compose`, but that brought in some `test.js` files (don't know why they'd include that), which confused Jest. Added `roots` key in `jest.config.js` to test only `src`.

My Splunk transport wasn't working, then I realized I needed to pipe to `pstBin`, not `pinoSplunkTransport`. Updated md file 15.1 so I can find it in the future.

So, the command line is `npm run express:dev:typeorm | APP_ENV=dev npx ts-node src/infrastructure/logging/pstBin.ts`.

But I'm getting an error from `SplunkLogger`. In `flushQueue`, when called from the timeout, `this._inQueue` is undefined. I changed the timeout to run `this.flushQueue.bind(this)`, though I'm not sure why it was losing context.

Splunk is working now.

Testing a request, I see that if the queue doesn't exist, I get an unreadably massive error. Added TODO item to reduce the error size. Meanwhile, created the queue. Restarting runs startup recovery, which posts the request to the queue. And `npx ts-node localQueue.ts read` shows the message.

So, I think my dev environment is working now.

**COMMIT: CHORE: get dev environment working; add Redis to docker**

## Interface communication channel to BullMQ

I don't have a component to receive data from the backup interface and relay it to the use case. Most of the parts exist, but I don't have the polling loop in written. I do have an example of a polling loop from previous work. I'll use that as a basis, but use the queue adapter for backup requests.

-  While not halt (set halt on `AbortController.signal`)
   -  Read from Azure Queue using the adapter
   -  If messages found, call message handler for each message
   -  If handler succeeds, delete message
   -  If handler fails and not a connect error and too many dequeues, send to error message handler (write to log, delete)
   -  If no messages found, delay (abortable, on abort, set halt)
-  Message handler
   -  `JSON.parse()` the message text
   -  If parse fails, error
   -  Assemble data for BullMQ job
   -  Add to BullMQ queue
-  Error handler
   -  Write message to log
   -  Delete message from queue

### What can fail?

-  Azure read can fail; I think I can ignore this and just delay
-  Azure delete can fail; more problematic because it can cause duplicates
   -  How can I track messages that failed to delete and ensure they're deleted; maybe check for them on "next" queues if I have enough history
-  Need to ensure that, if a Redis connect error failure we don't delete even if dequeue count is over limit
-  Need to ensure the loop halts on signal
-  `JSON.parse()` can fail; ensure error is handled correctly
-  Add to queue can fail; ensure it returns a connect error

What tests do I want for the queue watcher?

-  when start is called, the queue watcher moves to started status and the loop runs
   -  Check if loop runs by spying on `readMessages` call; expect to be called
-  when `AbortController` signals, the queue watcher loop stops and the queue watcher is in halted status
   -  Pass loop delay to the queue watcher; use a short delay, reset mock, halt, delay long enough for queue watcher to loop 2x, expect `readMessages` not to be called
-  when halt is called, the queue watcher is in halted status and the loop stops

## Work notes

Moved Azure Queue components to `src/infrastructure/azure-queue`, will build queue poller there. If I used a different backup interface infrastructure, it can go in a separate directory.

Moved queue read and delete methods out of `IBackupInterfaceStoreAdapter` to `IAzureQueueAdapter`

-  The queue watcher only cares about read and delete, IAQA lets us pass any read + delete implementer (for delete backup, restore backup, etc., in future)
-  `AzureBackupInterfaceStoreAdapter` implements both IBISA and IAQA, so can serve either role
-  I left implementations in ABISA because they need the circuit breaker, among other things, and I don't want to think through how to pass those around right now (maybe later)
-  Could I move send to IAQA and have ABISA call it from a function that maps for store data and calls IAQA function

Built tests and got the passing to prove loop control is working.

Added a test to confirm the message handler runs. Call message handler and prove test passes. Don't await message handler call because awaiting could delay event publish, which could let `popReceipt` expire before the subscriber has a chance to process the message.

**COMMIT: FEAT: add queue watcher; get basic functions working**

Built the server/app/init code to run the queue watcher and hook everything up so it should work. I'm getting errors reading the queue. I decided to try to use the debugger.

After some misleading search results, I found that I can run apps in the debugger with a script from `package.json`. In the Debugger panel, in the dropdown for the green arrow at the top of the panel, select `Node.js...`, then choose the script to run. Be sure to set a breakpoint before starting. Beware of breakpoints on HTTP calling functions because they'll timeout and fail. Set the breakpoint immediately after the HTTP calling function and examine results.

I made some Q&D changes to xBISA to deal with type conflicts while trying to get the debugger to run, will clean up later (added "NEXT" list below).

Not seeing queue HTTP errors anymore and the loop seems to be running happily.

I added a "send" option to `localQueue` and sent a success status message for a backup request in the table as Sent. I didn't set a breakpoint on the subscriber in time, so didn't see it run, but the request is updated and an instance created with data that matches the message. So, it works.

Changed `messageType` to `queueName` in `AzureQueueWatcher`. It makes more sense to log the queue name than a made up "message type".

**COMMIT: FEAT: build queue watcher runner code; see it work**

Built a queue helper API that can:

-  List all queues (`GET /queues`)
-  Get queue properties (`GET /queues/:queueName`)
-  Create a queue if it doesn't exist (`PUT /queues/:queueName`)
-  Delete a queue (`DELETE /queues/:queueName`)
-  Get messages in a queue (`GET /messages?queueName=:queueName`; be aware of visibility timeout and how it may affect results)
   -  parsedMessageText is `JSON.parse(messageText)` if parseable, {} if not parseable
-  Post a message to the queue (`POST /messages` with body `{ queueName: string, message: object | string }`)
-  Delete a message from the queue (`DELETE /messages` with body `{ queueName: string, messageId: string, popReceipt: string}`)
   -  The body must contain those three values but can contain other (ignored) values
   -  Easy use: get messages, then copy messageId and popReceipt with any data between them and paste into body

The API runs on env API_PORT + 5.

Exported Insomnia config to dev_notes; note that exported data assumes port 3005. Adjust if needed.

**COMMIT: FEAT: build queue helper API to make working with Azure queues easier**

Improve `getNextDelay`

-  options -> min, max, increment; always add increment until max
-  Delay management could be a lot more sophisticated, but this is good enough

Give receiver a port in `dev.env`

-  Renamed API_PORT to LINEBACKER_API_PORT
-  Added BRQW_ZPAGES_PORT
-  Added BRQW_START_WATCH_DELAY_MS for startup delay

In `initQueueWatcher`

-  pass a delay that will set the start delay for the queue watcher
-  move the start log into `AzureQueueWatcher`; add stop log too

Added getter for `queueName` to `IAzureQueueAdapter` and descendants so anything using the adapter can get the queue name if needed

Rename `expressServerTypeorm` to `expressApiSrvTypeorm` (etc.) for consistent naming

In `AzureQueue`, remove `request.operationSpec` from errors. This member is huge and has little diagnostic value.

Ensure unit tests pass.

While testing the application, I'm seeing a lot of ECONNRESET errors. Azurite seems to respond well to the queue helper API and many other places, but receive is not happy. When Azurite isn't hanging up sockets for unclear reasons, it works as expected. I trace the failure into AzureQueue and the call to `queueClient.receiveMessages`, which is the Azure SDK, so I'm not sure where the problem is, just know it's something in the Azure SDK's node-fetch that isn't happy. I even tried the Azurite npm package with a debug log. The debug log shows data for calls that get a response, but shows no data for calls that get ECONNRESET.

Send does not seem to have the same problem. I ran the API and fired off 37 requests, all of which hit the queue without issues--running from the VS Code extension which is very unresponsive to reads.

I added a `package.json` script to run `npx azurite` with my custom data location because that lets me see Azurite responses and seems to be more likely to respond than the Azurite VS Code extension.

I may try to trace this more in the future (try on another machine), but for now, this doesn't seem to be an issue in code I wrote.

**COMMIT: REFACTOR: improve queue watcher loop delays; better env and init; new and renamed package.json scripts; reduce size of SDK errors**

NEXT
