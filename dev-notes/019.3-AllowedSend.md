# Build events and realign use cases for check allowed and send

## Planning

I need a `BackupRequestAllowed` event. I don't think I'll need `BackupRequestSent`--at least not for now.

I also need

-  A use case for check allowed
-  A consumer for the received queue (calls check allowed)
-  A worker to run the consumer

-  A use case for send to interface
-  A consumer for the allowed queue (calls send to interface)
-  A worker to run the consumer

Which brings me to a major decision point.

I already have the use cases described, but they're aligned to the other event bus. I need to change them, and `AggregateRoot`, to use the new event bus. Later, I want to make the new event bus conform to the new event bus interface if possible so they can be swapped out.

How does the legacy event bus work?

-  Subscribers register with the `DomainEventBus` to be notified of events (`subscribe`)
   -  DEB has a handlers map, keyed by event name with values that are an array of handler methods
-  `AggregateRoot` has an array of domain events and methods to get, add, and clear domain events
-  Aggregates add events to their domain event array as part of processing
-  Methods on the aggregate add events as needed; for example, looking at `BackupRequest`
   -  `create` adds a created event if called with no id
   -  `setStatusChecked` adds an allowed event if the status is allowed
-  The repo `save` method calls `publishEventsForAggregate` if successful
-  `publishEventsForAggregate` loops over the aggregate's event array; for each event, it calls `publishToSubscribers` for all subscribers
   -  publishing means calling each handler method registered for the event

I haven't seen any cases where an event has more than one subscriber or an aggregate action results in more than one event. Those cases are theoretically possible, but don't exist in the foreseeable future.

BullMq doesn't support the idea of consumer groups--if more than one worker is consuming a queue, it is competing with other workers for jobs. To allow different types of workers to handle the same events, I'd need to publish the event to more than one queue. In that case, the event would need an array of queue names or need to be mapped to an array of queue names and `publish` would need to loop over the list of queue names to publish the event to all of them.

Kafka supports more than one consumer group, so Kafka would publish to one topic. The same basic strategy would work, though--just never more than one topic per event.

DEB supports many subscribers to an event. Publish would look a lot like the existing publish process. The existing process includes an array of marked aggregates and looks up aggregates by id in that array. I think it would be better to pass the aggregate to a method that publishes for the aggregate and then publish each event in the aggregate's event array to all subscribers.

That same strategy could work for other the new event bus or a Kafka-based bus. A `publishForAggregate` method that takes an `AggregateRoot`, gets the events from it and publishes those events. The specific aggregate can add events to the aggregate.

The DEB publish action happens in the `save` method on the repo. I've pulled the new event bus publish into the use case.

-  I could put publish in the repo, but the repo would need access to the event bus to make that work
-  I could make the event bus a singleton--have the module return the instance so there's only one in the application
-  The `save` method will need to return errors that may happen on the event bus in that case

I think moving into `save` is a good direction, but I want to get the other use cases working first, so I'll add it to the list of TODOs.

## Move to single queue for BullMq

I'm going to move the application to a single queue model--at least for BullMq.

-  Include event type in the job
-  Workers look at event type to decide which consumer method to call (`switch`)
-  This approach should reduce worker idle and support easier scaling
-  I'm going to leave topic name in the event for now--just set them all the same

### Notes

-  Write a consumer for `Received` that logs the received message
   -  I built a test use case so I could leave most of the consumer intact
-  Add event type to data the event returns
   -  Had to add to the event interface too
-  Change events to use the same queue (`'linebacker'`)
-  Add `switch` to worker and point it to new queue

Run the worker and post a request. The worker handles both events.

**COMMIT: REFACTOR: change to use a single queue; workers handle any event so are less idle**

## Backup request allowed

-  Add the event it will publish; same basic structure as Received
-  Change allowed use case
   -  I've put it in `use-cases/check-request-allowed-2` until I clean up the old event bus
-  Changed received consumer to use `CheckRequestAllowedUseCase`
   -  Was using a test use case, now use the real thing

Received consumer is passing tests. Running the app, it works.

Some unit tests failing--fixing added to local TODOs for next steps.

**COMMIT: FEAT: add event and use case for CheckAllowed; use in consumer**

**COMMIT: REFACTOR: rename events to match consumers; rename bmqWorker because it's runs any event**

**COMMIT: TESTS: fix broken tests and improve existing tests (spies, missed cases, etc.)**

## TODO

-  Send to interface
