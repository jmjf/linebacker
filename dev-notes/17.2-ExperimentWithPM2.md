# Experiments with PM2

## Goals

I want to run several sub-applications under PM2 and have it restart them if they fail. At a minimum, I want to separate the API from any queue watchers. I'd like to split parts of the API into separate processes (some branches may need more scalability than others, so scaling only those parts).

-  [x] Set up a config file that will run the API and the queue watcher
-  [x] Does PM2 require a binary (may present problems in some cases)
   -  It doesn't look like it from what I can see
-  What kind of process statistics data can I get from PM2
   -  [x] Can I build a simple health API that shows me process data for all processes
   -  Can I have processes update certain stats in the consolidated health view
   -  How do I handle liveness/readiess checks when different processes may have different requirements
   -  How to handle the idea of "ready to accept some requests but not others"
-  How do I support graceful shutdown of the processes
-  Can I accept requests on a single port and redirect to the correct process port based on route
   -  The idea is I might have a facade on a single port that distributes to different internal apps (ports) based on route
   -  Reasons
      -  If I want to split the API into different sub-APIs by route (e.g., backup request related, backup info related, delete backup actions, etc.)
      -  If I want to run more than one instance of a given sub-API for scaling
      -  If the reverse proxy in front of my application is out of my control
   -  Yes, but this may or may not be wise; may be better to get a proper container orchestrator and reverse proxy

## Setup

I want to run pm2 local, not global.

-  `npm install pm2`

Create a config file `src/test.pm2.config.js` [Ref](https://pm2.keymetrics.io/docs/usage/application-declaration/)

-  Filename must end in `.config.js`
-  I want to run with ts-node in dev, so I found a suggestion on how to set that up that may work
-  Set APP_ENV to dev

Early observations

-  The ts-node trick worked; API is running and responding
-  I don't see any evidence that PM2 is running a binary; doesn't mean it isn't, but if it is, it isn't obvious
-  Logs are redirected to a file, so need to figure out how to
   -  Not write to a file
   -  Redirect to Splunk or similar

Add the queue watcher as a separate app in the config file. That seems to be working too.

## Understanding the config file

I want to look at the config file options and think about what I want to use.

-  There's an `interpreter` option; I wonder if I can use that in place of the current ts-node trick
   -  Doesn't seem to work, so back to the the other way
   -  In staging and production, I would build and run from build anyway, so not a major issue
-  `instances` controls number of instances to run
-  Logging options don't seem to directly allow suppressing log files, but it looks like I could use the options to direct to `/dev/null`
-  `shutdown_with_message` sends a shutdown message to the processes; might be better than SIGINT because Windows is in scope
-  `wait_ready` will wait for `process.send('ready')` instead of a time or listen; might be a better option (set true)
-  `restart_delay` (ms) might be wise to avoid a tight crash/restart loop, especially if we're able to run more than one instance
-  `stop_exit_codes` may be useful for stop events that are intentional [Ref](https://pm2.keymetrics.io/docs/usage/restart-strategies/)

## Getting statistics

PM2 has a JavaScript API. I'll try to write a simple program to:

-  get a list of all applications and log it (`pm2.list`)
-  get details about a specific process (`pm2.describe`)
-  get details about all processes

`pm2.connect` to connect to PM2, then `pm2.list` works. Possibly interesting data:

-  Output is an array of objects representing processes
-  `pid`, `name`, `pm_id` (PM2 process id), `monit` (`{ memory, cpu }`)
-  `pm2_env` environment data, following might be interesting
   -  `autorestart`
   -  `name` (same as `name` above)
   -  `instances`
   -  `exec_mode`
   -  `APP_ENV` (environment variable I set on the command line)
   -  `npm_config_global_prefix` (seems to give node version, but may be thanks to `nvm` path)
   -  `npm_config_prefix` (similar to above)
   -  `npm_config_user_agent` (has npm and node versions in the string)
   -  `NODE` (path to node)
   -  `status` (seems to be in info about the process)
   -  `created_at` and `uptime` (epoch time)
   -  `restart_time` (restart count)
   -  `unstable_restarts` (concept in PM2 docs)
   -  `version` (from `package.json`)
   -  `node_version`
-  Other `pm2_env` members
   -  `versioning` seems to be about git stuff
   -  `env` is environment data; seems to already exist in `pm2_env`
   -  `args` from the config file
   -  `axm_actions` not sure what this is
   -  `axm_monitor` memory size, event loop latency, active handles, active requests
   -  `axm_options` seems to be metrics configuration

Given this info, I think I could write code to get this data and put it in an API endpoint (healthz?).

-  Connect, list, iterate list and build process data to show
-  Wouldn't have details of circuit breakers, etc.
-  May be able to use PM2 IPC to get updates
-  Or if I could promisify the PM2 methods with `util.promisify` and maybe send signals and wait for responses
   -  Would probably need to use `Promise.all()`, so would probably not use await
-  They seem to have a module `tx2` that allows some fancy custom metrics; should end up in `axm_monitor`, I think
   -  Challenge would be figuring out how to add this into the application without coupling it to pm2/tx2
   -  tx2 offers an `action` that is callable by pm2
   -  `pm2.trigger` examples in pm2 repo examples (tx2 = pmx ???)
      -  `pm2.trigger(pm_id: string, actionName: string, callback: (err, res) => {})`
      -  response seems to be in `res[0].data`; includes other data
      -  must call `pm2.disconnect` after ???
      -  if I can promisify this, I may be able to `Promise.all()` a map of calls for every process
      -  In the app, the action runs what healthz runs today

## Logging

Currently, I log by piping stdout to a transport. That doesn't seem to play nice with the config file `args`. I can make it run with a shell script, but that loses the memory monitoring and other details available about the process, which makes me suspect the actions plan above may not work. I suspect the script prevents access to the process.

Changed `pinoLogger` and "server" startups. `pinoLogger` now exports `buildPinoLogger`, which accepts a name for options so I can get different names for different parts of the application (api, queue watcher, etc.). Which doesn't solve the problem because many things import `logger`. ~~For now, revert and add to TODO list.~~

Found a solution. `logger.setBindings()` lets me pass an object that will be added to the logger. Because `logger` is a singleton/global, I can import wherever I want to use it and in the server startup, set bindings with a `service` and `feature`, which gets added to all logs, even those from components that directly import logger (confirmed by running and seeing `DomainEventBus` logs include the extra data). So, i can search logs by name (linebacker for all components), service (api, queue-watcher, etc.) and feature (store, delete, etc.) to narrow down results as needed.

Based on some issues on GitHub (pino), it looks like PM2 monkey patches stdout, so to get logs working, I'll need to pass the transport in `pinoOptions`. Tested this with `pino-pretty` and it seems to work, but PM2 logs seem to be super wonky.

-  `pm2 logs` shows data and claims it is reading from a specific file, but when I `cat` that file, nothing is there
   -  The logs shown this way are run through `pino-pretty`, so embedding the transport works
   -  The transport must be `js`, so I'll need to compile to use the module; probably will npm-ify it
-  `pm2 logs --raw`, which is what I'd need for Splunk piped from the console, shows no data
-  No variation of `pm2 logs` streams data like the docs and several threads I've seen suggest it should
-  `pm2 monit`, which showed log data in the past, shows nothing, though that may be because I'm merging logs and confusing it
-  Even removing my log customizations (merge, send both error and output to the same file for all processes), it doesn't behave the way I'd like

The only solution I see right now is to select the transport when I build the logger, which will mean configuration/environment values to make that happen.

Found the issue. I found pm2 was executing in cluster mode instead of fork mode, probably because I have instances specified. I'm sure tests last night were in fork mode, but I see a lot of terminal history from today showing cluster. I changed the config file to fork. Now `pm2 logs --raw` works and is streaming, even when `npx`ed, and monitor is showing logs. `npx pm2 logs --raw` is showing all logs too, so I could run a script that would pipe to a pino transport maybe. Last night, that froze the machine, but I think I may be able to avoid that by sending the logger process's output to `/dev/null`.

Yes, that approach works. I need to tune it so the logger starts first so no logs are missed, but it works. I wrote `pstBin` to log to both Splunk and the console, so it was logging to itself in a loop, but not out of control like last night. In the shell script that runs `pm2 logs | pstBin`, add `> /dev/null` to prevent that.

BUT: It doesn't work in cluster mode, no matter what I try. For cluster mode, I need to include the transport in the application. I'm back to, pm2 logging in cluster mode is seriously broken--but at least I have a workaround. It would be nice if pm2 would let me say disable logging or route logs to the real stdout.

I also confirmed that shell scripts do not provide `axm*` values, so monitoring ability is limited.

## Cluster mode

Besides the logging issues, note that cluster mode with more than one instance doesn't play nice with npm--specifically, it doesn't allow port sharing. So, to use cluster mode with more than one instance of a given app, I need to compile the code.

Which also solves my cluster mode log issues--lesson learned: pm2 clustering is a production-like feature.

I found some challenges related to `FakeAuthNZ`. For now, I'm including `test-helpers` in `dist`, but I need to find a better way to handle that. Added to TODO list.

I also want to get the pm2 process id and add it to the logger bindings. pm2 sets an environment variable, `NODE_APP_INSTANCE`, with the process number. I'll rename it it `PM2_INSTANCE_ID` in the config file to make it clearer. But, that's the instance number. I also want the pm2 process id, which is `process.env.pm_id`.

**COMMIT: FEAT: add pm2-awareness to logs; get pm2 basically working (more to do)**

## Health API

I want to build an API that provides health information for processes running under PM2. Do it as a zpages item -- pm2healthz.

The basic approach should be as outlined below. The `return`s will likely become responses

```javascript
// promisify pm2's callback taking functions
const pm2Connect = util.promisify(pm2.connect);
const pm2List = util.promisify(pm2.list);

let errorFrom = '';
try {
   errorFrom = 'connect';
   await pm2Connect();

   errorFrom = 'list';
   const pm2Processes = await pm2List();

   pm2.disconnect(); // does not take a callback

   return pm2Processes.map((item) => {
      const {name, pm_id, pid, monit, pm2_env} = item;
      return {
         name,
         pm_id,
         pid,
         memory: monit.memory,
         cpu: monit.cpu,
         instances: pm2_env.instances,
         exec_mode, APP_ENV, status,
         created_at, uptime,
         restartCount: restart_time,
         version, node_version
         axm_monitor
      }
   });
} catch (e) {
   return { message: `pm2 ${errorFrom} error`, error: err };
}
```

I couldn't use `util.promisify` because `pm.connect()` (and possibly other things) refers to `this`. It looks like `utils.promisify` loses `this`. I built a custom `promisify(f, theThis)` that accepts the correct `this` to use (`pm2`). Then I can `const pm2Connect = promisify(pm2.connect, pm2)` and it works. The main downside is, it loses types, so I need to cast return values. That only affects `pm2List`, which returns `pm2.ProcessDescription[]`, so is a small issue.

Now, I want to get a consolidated `healthz` that shows all processes `healthz`. Can I use [tx2](https://github.com/pm2/tx2) to create actions, then use `pm2.trigger(pmId: string, actionName: string, callback: (err, res) => void))` to call a `getHealthz` for each process. One challenge is, pm2 types don't know about `trigger` so I need to do something like below--and, ideally, promisify it.

```typescript
// eslint-disable-next-line @typescript-eslint/ban-ts-comment
// @ts-ignore
// eslint-disable-next-line @typescript-eslint/no-unused-vars, @typescript-eslint/ban-types
const pm2Trigger: (pmId: string, actionName: string, params: unknown, cb: Function) => void = pm2.trigger;
```

**COMMIT: FEAT: add pm2healthz to zpages**

I'm not sure if I should use `tx2` or `@pm2/io`. I'm going with `tx2` because it was updated more recently, but note that `@pm2/io` is included with `pm2`, so may be an alternative.

`npm install tx2`

`npm install --save-dev @types/tx2`

Now, let's configure an action in the zpages module.

-  Import `tx2`
-  Write a custom promisified version of `pm2.trigger` (`pm2Trigger`)
-  Build a function that returns the data `healthz` returns
-  Add a `tx2` action named `get-healthz`
   -  It takes an action name and a callback
   -  The callback just returns `getHealthz()`
-  Add a route for `all-healthz`
   -  It `await`s `pm2Trigger` and responds with the result or a 500 error

And let's see if it works.

It basically works. For each item in the result, the action includes the process data shown below, `at` (an epoch time), and puts the data in `data.return`. `data` also has an `action_name`, which I don't need

```json
"process": {
   "namespace": "default",
   "rev": "c5214a763b059460a6509a0a34cac4aacac0b1e5", // commit SHA, but if not in a repo, not sure what it will be
   "name": "store-queue-watcher",
   "pm_id": 1
}
```

I want to restructure that so data is merged into the outer object.

-  Declare an interface so I have a definite return type for trigger
-  `map()` the data from `pm2Trigger` to reformat it
-  Convert `at` to an ISO string

I see that the api process has a `process.versioning` instead of `process.rev`, with a pile of git related data. I'll pare that down to `rev`, which will be `process.versioning.revision`. And I'll sort by `pm_id`.

I want to figure out how the action gets the `process` data and add that to the `healthz` result (in the route). Knowing which process is responding might be helpful. After some digging, it gets it from `pm2_env`. I can get `pm_id` from `process.env.pm_id`. Looks like `name`, `versioning`, and other useful info is there too, so plan to pull that into vanilla `healthz`.

**COMMIT: FEAT: add all-healthz (returns health data for all processes) and add process data to healthz**

## Other notes

-  A possible solution to my startup recovery challenge and the desire to run more than one instance
   -  The main place I might want more than one instance is the Receive (http api, handle more requests)
   -  Currently, everything is in one server and startup recovery runs for all cases at startup
   -  If CheckAllowed and SendToInterface have their own processes, I can run one of each and they can startup recover their piece
   -  Then I can run many Receives; still limited to one CheckAllowed and one SendToInterface, but they aren't user-facing
   -  I still want to look at a queueing solution, but this is another option to consider
